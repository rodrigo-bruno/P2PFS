\documentclass[times,9pt,article]{llncs}
\usepackage{times}
\usepackage{makeidx}

\begin{document}
\title{Peer-to-Peer File System}
\institute{Peer-to-Peer Systems and Overlay Networks \\
Masters Degree in Telecommunications and Informatics Engineering \\
Instituto Superior T\'ecnico}

\author{Group Number 2 \\
Jo\~ao Granchinho n.54766 joao.granchinho@ist.utl.pt \\
Pedro Torres  n.63506 pedro.torres@ist.utl.pt \\
Rodrigo Bruno n.67074 rodrigo.bruno@ist.utl.pt}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 1:  Introduction 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In this document we will be describing our solution and explaining how we intend
to fulfill the challenge proposed by the project specification.\\
We will start by explaining why we chose Kademlia as our DHT for this project and
 then we detail the protocol that we will be using for implementing a distributed
file system on top of a Kademlia peer-to-peer network overlay.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 2:  Kademlia 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kademlia}

As one of the studied DHTs and one of the proposed DHT implementations for the 
project, we decided to use TomP2P, a Kademlia's protocol implementation.\\
This decision was mainly motivated by the fact that Kademlia was designed to be
used by file sharing applications and therefore provides some nice features that
will be very helpful for our file system's implementation. We will now describe
 some of the Kademlia features and explain how we will take 
advantage of them. 

\subsection{Iterative Parallel Search}
Kademlia and therefore TomP2P uses iterative parallel search. Two main benefits
from this search procedure are: 1) generated/received information is useful for
 refreshing the k-buckets; 2) parallel queries prevents waiting for timeouts to
 detect failed nodes and allows the fastest nodes (the ones with the lowest RTT)
 to be used. As that being, using Kademlia we will be able to provide a better 
 quality of service by providing faster search and reduced maintenance traffic.

\subsection{Key-Republishing}
Key-Republishing is a very interesting feature and it is important to ensure the
 persistence of the key-value pairs. Two phenomena may 
jeopardize the key-value pairs: a node responsible for the pair leaving the 
network and a node with a closer id (closer to the key) joining the network. \\
TomP2P takes care of both scenarios using Indirect Replication. The activation 
of this mechanism will ensure that nodes react when one of the situations above 
described happen. This Key-Republishing will be very useful since it will help 
implementing file replication algorithm that is one of the requirements for our 
file system.   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 3:  P2P FS Management 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Gossip}

With the purpose of monitoring P2P FS and provide high level information to the 
users, we will use a Gossip protocol, where each node will exchange information
with the nodes it knows from its k-buckets.\\
To be able to answer the proposed queries, each node will maintain the following
set of values, which will be exchanged with the other nodes it knows: Sn, Su, Sa,
W1, Ss, Sm, W2. The protocols that will calculate the answer to these queries are
as follows:
\begin{itemize}
\item number of nodes running P2P FS:
Each node will have a value of Sn = 1. And a sum is performed by gossip, where each 
node will send to a neighbour half of its value and half of its weight, keeping the 
other halves to itself. The result is Sn/W1.

\item number of users:
To know the number of users we apply the same algorithm as before, except now 
the value Su will simply be the number of metadata lists. For each node we see if
that node contains any user keys. Since the keys themselves will be replicated,
 after the return of the request (which will be the result of the gossip) all
 there's left to do is divide by the replication factor to get an approximated
 number of users. So the result is (Su/W1)/Rep.

\item number of active users (with their FS mounted):
To know the number of active users we apply the same algorithm seen so far, with a
value Sa which will be equal to the number of user keys that node maintains with
mounted dirs. Same as before, because of the replication after the return
we just need to divide this number by the replication factor: (Sa/W1)/Rep.

Since performing a sum or counting uses the same values for weight, for queries 1,2 
and 3 the weight used will be the same (W1), and the exchanges for these 3 queries 
can be sent in a single message (ex: \textless Sn, Su, Sa, W1\textgreater). For these algorithms to work 
we need to ensure the weight of the total nodes sums up to 1. To do this we aim to
have X bootstrapping nodes in the initial network. Each of these nodes will have 
W1=1/X. 
Any new nodes that join always have W1=0. The weight will end up being distributed 
among the new peers and thus diluted, making the total sum equal to 1.

\item average number of files stored:
To perform an average the algorithm is similar to the one used so far. Each node
sends half its value Ss and half its value W2 to another, except this time every 
node starts with W2 = 1. To get the Ss, each node will get the number of files 
stored by counting the files maintained. Since all the files are replicated, after
all nodes converge, the average will be (Ss/W2)/Rep.

\item average amount of MB stored per node:
For this query, just like the query before, each node sends half its value Sm and 
half its value W2 to another, each node having initially W2 = 1. Sm for a given node
will be the sum of all the files MB in a given node. The result will be Sm/W2. Just
like before, after all nodes converge, the average will be (Sm/W2)/Rep due to the 
replication.

Since these last 2 queries perform averages, and their weights are therefore the same,
it makes sense to exchange both requests at the same time, just like before:
\textless Ss, Sm, W2\textgreater.

Whenever any of the values (S's) changes internally on a given node (either because
the number of metadata lists changed, or the number of lists with mount flags changed,
etc.) the difference will be added/subtracted from the current value but the weight 
stays the same and the algorithm resumes normally.
Since the gossip algorithms converge after a while, we will need to test various
gossiping exchange intervals and find one that is acceptable both in bandwidth and
convergence time.

\end{itemize} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 4:  File System Metadata
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{File System Metadata}


In P2P FS, all metadata is stored as files. For better understanding, we will
present its main structure:
\begin{itemize}
\item \textless \emph{id}, \textless \emph{type}, \emph{user}\textbar \emph{file}\textbar \emph{directory}\textbar \emph{part}\textgreater \textless \emph{rest}\textgreater\textgreater ;
\end{itemize}

Note that TomP2P is able to store multiple values for each key (given an 
auxiliary key). Therefore, the \emph{rest} field will be a set of secondary
keys and respective values.

\begin{itemize}
\item if \emph{type} == \emph{user} then \emph{rest} has two sec-keys: \emph{home} (for accessing files in the user's home directory) and \emph{mounts} (number of mounts in node);
\item if \emph{type} == \emph{file} then \emph{rest} has three sec-keys: \emph{nparts} (number of parts of the file), \emph{content} (access data) and \emph{hash};
\item if \emph{type} == \emph{directory} then \emph{rest} is the same as in files;
\item if \emph{type} == \emph{part} then \emph{rest} the same as files without the \emph{nparts} key.
\end{itemize}


Files stored on P2P FS will have specially constructed ids (that will be used to
generate keys) obtained by using the following information: owner user, complete 
path, file type and part number.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 5:  Load Balancing/Security
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Load Balancing/Security}

Load balancing and security are two of our project's requirements. This two 
requirements can be met by splitting files into multiple nodes:
\begin{itemize}
\item load balancing will be achieved by using a hash function that will take
an id and output a new key that will be mapped into a different node. This way,
when a file gets bigger that a certain amount of kBs, the file gets split and
therefore, the load for accessing the file will be shared;
\item security concerns will also be met by file division since the files will
tend to be divided in several machines. Therefore, no node will host all file
parts.
\end{itemize}
The used criteria for splitting a file is a fixed size limit. When the last part
of a file gets bigger than the limit, that file gets split. By using a not so high
value we facilitate the distribution of load among all nodes. However this value
is application dependent and for developing purposes we will start by using a
1Mb part size limit.   

\section{Other Design Decisions}
\begin{itemize}
\item We define that users will host files when they stay connected for a long span of time (for example for more that 15 minutes);
\item Local caching of the user's files will be done to improve search efficiency.
To this end, we will have to delay writes and reads (for maximizing the cash usage).
\item For developing purposes we will start by using a replication factor of 3. If we feel that the nodes in the network
have a very high churn we will be forced to increase the replication factor (this will instroduce extra overhead).
\end{itemize}

\end{document}
