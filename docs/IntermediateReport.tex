\documentclass[times,9pt,article]{llncs}
\usepackage{times}
\usepackage{makeidx}

\begin{document}
\title{Peer-to-Peer File System}
\institute{Peer-to-Peer Systems and Overlay Networks \\
Masters Degree in Telecommunications and Informatics Engineering \\
Instituto Superior T\'ecnico}

\author{Group Number 2 \\
Jo\~ao Granchinho n.54766 mail@ist.utl.pt \\
Pedro Torres  n.63506 pedro.torres@ist.utl.pt \\
Rodrigo Bruno n.67074 rodrigo.bruno@ist.utl.pt}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 1:  Introduction 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In this document we will be describing our solution and explaining how we intend
to fulfill the challenge proposed within the project specification.

We will start by explaining why we chose Kademlia as our DHT for this project.
Next we detail the protocol that we will be using for implementing a distributed
file system on top of a Kademlia peer-to-peer network overlay.

Conclusion?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 2:  Kademlia 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kademlia}

As one of the studied DHT and one of the proposed DHT implementations for the 
project, we decided to use TomP2P, a Kademlia's protocol implementation.

This decision was mainly motivated by the fact that Kademlia was designed to be
used by file sharing applications and therefore provides some nice features that
will be very helpful for our file system's implementation. 

We will now describe some of the Kademlia features and explain how we will take 
advantage of them. 

\subsection{Iterative Parallel Search}
Kademlia and therefore TomP2P uses iterative parallel search. Two main benefits
from this search procedure are: 

\begin{itemize}
\item generated/received information is useful for refreshing the k-buckets;
\item parallel queries prevents waiting for timeouts to detect failed nodes and 
allows the fastest nodes (the ones with the lowest RTT) to be used. 
\end{itemize} 

As that being, using Kademlia we will be able to provide a better quality of
service by providing faster search and reduced maintenance traffic.

\subsection{Key-Republishing}
Key-Republishing is a very interesting feature provided by Kademlia. This feature
is important to ensure the persistence of the key-value pairs.

Two phenomena may jeopardize the key-value pairs: a node responsible for the pair
leaving the network and a node with a closer id (closer to the key) joining the
network. 

TomP2P takes care of both scenarios using Indirect Replication (name used in the
TomP2P's documentation). The activation of this mechanism will ensure that nodes
react when one of the situations above described happen. 

This Key-Republishing will be very useful since it will help implementing file
replication algorithm that is one of the requirements for our file system.   

\subsection{Caching (direct replication)}

Another interesting feature is caching. Caching enables the replication of 
key-value pairs along the search path. If we take in consideration that our 
distance metric (exclusive or) is unidirectional, this means that searches 
from different origins will converge into a search path. As that being, using
caching, Kademlia is able to reduce the burden on nodes holding popular content.

Within the context of our project, this functionality can be used to speed up
the statistical information delivery. Since this information will be calculated
periodically, its contents can be cached (temporarily) to provide lower load on
the node holding this information.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 3:  P2P FS Management 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Gossip}

With the purpose of monitoring P2P FS and provide high level information to the 
users, we will use a Gossip protocol, where each node will exchange information
with the nodes it knows from its k-buckets.\\
To be able to answer the proposed queries, each node will maintain the following
set of values, which will be exchanged with the other nodes it knows: Sn, Su, Sa,
W1, Ss, Sm, W2. The protocols that will calculate the answer to these queries are
as follows:

\begin{itemize}

\item number of nodes running P2PFS
Each node will have a value of Sn = 1. And a sum is performed by gossip, where each 
node will send to a neighbour half of its value and half of its weight, keeping the 
other halves to itself. The result is Sn/W1.

\item number of users
To know the number of users we apply the same algorithm as before, except now 
the value Su will simply be the number of metadata lists. For each node we see if
that node contains any metadata lists, each list corresponds with a user. Since 
the lists themselves will be replicated, after the return of the request (which will
be the result of the gossip) all there's left to do is divide by the replication 
factor to get an approximated number of users. So the result is (Su/W1)/Rep.

\item number of active users (with their FS mounted)
To know the number of active users we apply the same algorithm seen so far, with a
value Sa which will be equal to the number of lists that node maintains with the
mounted flag. Since the metadata lists will contain information about that user's
FS mount state, all that's needed is to count those lists to get the number of
active users per node. Same as before, because of the replication after the return
we just need to divide this number by the replication factor: (Sa/W1)/Rep.

Since performing a sum or counting uses the same values for weight, for queries 1,2 
and 3 the weight used will be the same (W1), and the exchanges for these 3 queries 
can be sent in a single message (ex: <Sn, Su, Sa, W1>). For these algortihms to work 
we need to ensure the weight of the total nodes sums up to 1. To do this we aim to
have X bootstrapping nodes in the initial network. Each of these nodes will have 
W1=1/X. 
Any new nodes that join always have W1=0. The weight will end up being distributed 
among the new peers and thus diluted, making the total sum equal to 1.

\item avg number of files stored
To perform an average the algortihm is similar to the one used so far. Each node
sends half its value Ss and half its value W2 to another, except this time every 
node starts with W2 = 1. To get the Ss, each node will get the number of files 
stored by counting the files maintained. Since all the files are replicated, after
all nodes converge, the average will be (Ss/W2)/Rep.

\item avg amount of MB stored per node
For this query, just like the query before, each node sends half its value Sm and 
half its value W2 to another, each node having initially W2 = 1. Sm for a given node
will be the sum of all the files MB in a given node. The result will be Sm/W2. Just
like before, after all nodes converge, the average will be (Sm/W2)/Rep due to the 
replication.

Since these last 2 queries perform averages, and their weights are therefore the same,
it makes sense to exchange both requests at the same time, just like before:
<Ss, Sm, W2>.

Whenever any of the values (S's) changes internally on a given node (either because
the number of metadata lists changed, or the number of lists with mount flags changed,
etc) the difference will be added/subtracted from the current value but the weight 
stays the same and the algortithm resumes normally.
Since the gossip algorithms converge after a while, we will need to test various
gossiping exchange intervals and find one that is acceptable both in bandwidth and
convergence time.

\end{itemize} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 4:  File System Metadata
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{File System Metadata}

In P2PFS there are two types of entities: users and files . Entities representing
users will use the user name as key (hash of the user name) and the value will 
be the number of active mount points. 

Files will represent both normal files and directories. Both entities will have 
a number of parts and a summary (checksum). For files, the actual file content
is stored; for directories, the available files within will be stored. Only the
first part of each file needs to store the number of file parts. Note that 
TomP2P is able to store multiple values for each key (given an auxiliary key).   

Files stored on P2PFS will have specially constructed ids (that will be used to
generate keys) obtained by using the following information: owner user, complete 
path, file type and part number.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 5:  Load Balancing/Security
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Load Balancing/Security}

Load balancing and security are two of our project's requirements. This two 
requirements can be met by splitting files into multiple nodes:
\begin{itemize}
\item load balancing will be achieved by using a hash function that will take
an id and output a new key that will be mapped into a different node. This way,
when a file gets bigger that a certain amount of kBs, the file gets split and
therefore, the load for accessing the file will be shared;
\item security concerns will also be met by file division since the files will
tend to be divided in several machines. Therefore, no node will host all file
parts.
\end{itemize} 

The used criteria for splitting a file is a fixed size limit. When the last part
of a file gets bigger than the limit, that file gets split. By using a not so high
value we facilitate the distribution of load among all nodes. However this value
is application dependent and for developing purposes we will start by using a
1Mb part size limit.   

\section{Other Design Decisions}
\begin{itemize}
\item As an optimization method we define that users will store its files when they stay connected for a long span of time;
\item Another optimization relates to local caching of the file system to improve search efficiency. As well as periodic Reads of node metadata to update information and the delay of Writes to avoid collisions.
\end{itemize}

\section{Semantics}
We provide some semantics to allow a better understanding of our system:
\begin{itemize}
\item \textless \emph{id}, \textless \emph{type}, \emph{user}\textbar \emph{file}\textbar \emph{directory}\textbar \emph{part}\textgreater \textless \emph{rest}\textgreater\textgreater ;
\end{itemize}
Depending on the \emph{type} we have different semantics where the rest varies:
\begin{itemize}
\item if \emph{type} is the \emph{user} then the  rest is \textless \textasciitilde /\textgreater, files in the user's home directory, \textless \emph{mounts}\textgreater, number of mounts in node;
\item if \emph{type} is a \emph{file} then the  \emph{rest} is \textless number of parts\textgreater , number of parts of the file, \textless complete hash of the file\textgreater;
\item if \emph{type} is a \emph{directory} then \emph{rest} is \textless list of files in the \emph{directory}\textgreater;
\item if \emph{type} is a \emph{file part} then \emph{rest} is \textless \emph{content}\textgreater , content of file\textgreater , \textless \emph{hash}\textgreater;
\end{itemize}
The \textless \textasciitilde / is maintained only to facilitate access to the files in the user's home directory as a list commonly seen as a result of the Unix command \textbf{\emph{ls}}.

\section{Future Enhancements}


\end{document}
