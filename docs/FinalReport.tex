\documentclass[times,9pt,article]{llncs}
\usepackage{times}
\usepackage{makeidx}
\usepackage{algorithm2e}

\begin{document}
\title{Peer-to-Peer File System}
\institute{Peer-to-Peer Systems and Overlay Networks \\
Masters Degree in Telecommunications and Informatics Engineering \\
Instituto Superior T\'ecnico}

\author{Group Number 2 \\
Jo\~ao Granchinho n.54766 joao.granchinho@ist.utl.pt \\
Pedro Torres  n.63506 pedro.torres@ist.utl.pt \\
Rodrigo Bruno n.67074 rodrigo.bruno@ist.utl.pt}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 1:  Introduction 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In this document we will be describing our solution and explaining how we fulfilled the challenge proposed by the project specification.\\
We will start by explaining the problems faced, as well as detailing the project requirements during the implementation of this project and then we detail the protocol that we used for implementing a distributed
file system on top of a Kademlia peer-to-peer network overlay. Followed by an evaluation of the system and an explanation of all our choices made in the duration of this project.

% mention that the report will be divided in: P2P File System and Gossip Algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section:  Problem statement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Statement}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section:  Protocol Description
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Protocol Description}

\subsection{Peer to Peer File System}
In order to better understand all the system behavior and associated protocol, it is important  to note first, the system's architectural division. Our system is divided in several layers:

\begin{itemize}
\item \textbf{FUSE API implementation}: this layer is responsible for receiving and processing all that is sent by FUSE (the upper layers that is outside our system);
\item \textbf{File System Cache}: this layer is responsible for delaying persistent writes and for caching updated file blocks;
\item \textbf{DHT Bridge}: this layer abstracts the basic operations provided by the DHT. It is used to enable clients to be part of the DHT or not;
\item \textbf{Host}: this is the bottom layer for our system. It is like a small server that receives requests from any number of clients (including the local ones) and injects them into the DHT.
\end{itemize}

Now we will describe in detail the goals and the protocol encapsulated by each one of the presented layers.

\subsubsection{FUSE API Implementation}

As presented before, this is where our system handles the FUSE calls to retrieve and store information. We believe that the most interesting part of this layer is the algorithm that converts index based access to files into block accesses. Although the implementation was hard to get it right, the algorithm is very simple:

\begin{enumerate}
\item Given a file block size and the index interval to access, it is possible to calculate the first and the last block where the read or write will take effect;
\item Having the first and the last block indexes, we fetch all the blocks between the first and the last (including these two off obviously);
\item Once all the blocks have arrived, we copy all of them into a larger buffer that will be used to perform the read or write;
\item When the FUSE operation is done, the reverse operation is done, we take a large buffer (and the first and last block indexes) and we split it into blocks that will be pushed to the DHT (using the Cache).
\end{enumerate}

\subsubsection{File System Cache}

This is perhaps the most interesting layer inside the FS related part of the project. By using a simple cache it was possible to obtain a huge boost in the FS performance.

This cache has two purposes: 
\begin{enumerate}
\item \textbf{holding writes}: this is very important, since file access (reads and writes) are usually sequential and by that, we can assume that if a block was created just now, it will most probably be accessed in the near future (this is the temporal locality principle);
\item \textbf{keeping a local copy}: this means that when we have a write or a read, and some blocks go through the cache, they will stay there until they are declared out dated and replaced by new blocks.
\end{enumerate}


The basic protocol for handling read requests is the following: if the requested object is on cache, the hash is checked. If it matches, return it, otherwise use the DHT to retrieve the object, store it and then return it.

For handling write requests the protocol is even more simple: we just store the object on cache.

The interesting part of the protocol is executed periodically to refresh all the cache objects. It goes as follows:

\begin{algorithm}[H]
 $TIC \longleftarrow maximum\ time\ in\ cache$\;
 $MBF \longleftarrow maximum\ number\ of\ block\ flushes\ per\ iteration$\;
 \For{$all\ the\ cached\ objects$}{
  $ttf \longleftarrow time\ to\ flush -\ refresh\ interval$\;
  $tic \longleftarrow time\ in\ cache +\ refresh\ interval$\;
  $mbf \longleftarrow 0$\;
  $dirty \longleftarrow was\ the\ file\ modified?$\;
  \If{$tic\ >=\ TIC\ and\ ttf <=\ 0$}{
   \If{$object\ is\ file\ block\ and\ mbf\ <\ MBF\ and\ is\ dirty$}{
    write block to DHT\;
    $dirty \longleftarrow false$\;
    $mbf++$\;
   }
   \ElseIf{$object\ is\ Metadata$}{
    write Metadata to DHT\;
    remove from cache\;
   }
  }
 }
\end{algorithm}

To complement the algorithm some important details will follow:

\begin{itemize}
\item for file blocks, the \emph{get} method will check if the block is cached and if so, it will check the hash. If it doesn't match, we need to re-fetch the block and cache it;
\item for each access (read or write) the time to flush the object is incremented to its maximum value. This represents that the block is being accessed and we would like to delay the write as much as possible;
\item file blocks are never removed from cache until a \emph{get} request detects that it is out dated;
\item metadata objects are always removed from cache to force the client to re-fetch the metadata periodically. This ensures that we will have always updated block hashes.
\end{itemize}

\subsubsection{DHT Bridge}

The DHT bridge layer is used to enable clients to be connected to a remote Host (a node inside the DHT). This way, a client can be connected directly to the DHT or use some other node as access point for the DHT. We will later explain (in Implementation Choices) why we use this bridge.

\subsubsection{Host}
Our final layer is the one we called Host layer. This layer is ruled by three objectives: 1) being an access point to DHT (accepting connections from clients, including the local ones); 2) be a member of the DHT to operate requests from clients and to help replicating files; 3) perform the gossip algorithm (explained later in much detail).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section:  Evaluation of the Protocol (strengths and weaknesses)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Critical Evaluation of the Protocol}

Regarding the critical evaluation of the developed protocol, we will point out some aspects that we believe to be significant strengths or weaknesses.

Starting by the division of files by blocks. Dividing a file in several blocks allows us to only fetch the blocks needed for some operation instead of storing and reading the complete file at once. This is a big advantage in terms of speed and used bandwidth. However, in a simple implementation like ours, we incur in some waste of space. This is mainly due to the fact that we use fixed length blocks. As that being the last block of each file, or a file with just one block may not be filled up completely. The bigger the file block is, the bigger the wasted space becomes.

The cache is another important feature. It allows us to write and read much faster. We are also able to throttle the write rate to make sure that the user will not get stuck after writing a file. However, the downside of this is the time to converge. If the same user wants to have multiple mounts, he/she must wait until all the dirty blocks from the cache have been flushed. This of course depends on the write throttling. Nevertheless, we assume that the same client will not be in two places at the same time and therefore, it will take some time to remount the file system elsewhere. Other solution is to use some indicator (like Dropbox does) to warn the user when files are still being updated. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section:  Implementation Choices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation Choices}

We will know describe some decisions that we took and that were fundamental to achieve the solution that we are presenting.

\subsection{Kademlia}
As one of the studied DHTs and one of the proposed DHT implementations for the project, we decided to use TomP2P, a Kademlia's protocol implementation.\\
This decision was mainly motivated by the fact that Kademlia was designed to be used by file sharing applications and therefore provides some nice features that will be very helpful for our file system's implementation. We will now describe  some of the Kademlia's features and explain how we will take advantage of them.

\subsubsection{Iterative Parallel Search}
Kademlia and therefore TomP2P uses iterative parallel search. Two main benefits from this search procedure are: 1) generated/received information is useful for  refreshing the k-buckets; 2) parallel queries prevents waiting for time-outs to  detect failed nodes and allows the fastest nodes (the ones with the lowest RTT)  to be used. As that being, using Kademlia we will be able to provide a better  quality of service by providing faster search and reduced maintenance traffic.

\subsubsection{Key-Republishing}
Key-Republishing is a very interesting feature and it is important to ensure the  persistence of the key-value pairs. Two phenomena may jeopardize the key-value pairs: a node responsible for the pair leaving the network and a node with a closer id (closer to the key) joining the network. \\
TomP2P takes care of both scenarios using Indirect Replication. The activation of this mechanism will ensure that nodes react when one of the situations above described happen. This Key-Republishing will be very useful since it will help implementing file replication algorithm that is one of the requirements for our file system.

\subsection{Clients Joining the DHT}
Other important aspect of our implementation is the possibility of clients to be inside or outside the DHT. In our current implementation, clients will join the network after being connected for some time (15 minutes for example). Our first motivation for this feature was to avoid high client churn (since it has been proven that the more time a node is connected the higher the probability of it being connected for a long time). This feature could also be interesting for mobile devices where one could access his/her files without the overwhelming overhead of being connected to the DHT.

\subsection{File block size and Write throttling}
We believe that these are two fundamental parameters to achieve a proper implementation. File block sizes bigger than 4K (the usual write/read buffer used by FUSE) will act as pre-fetching and thus will increase the performance (spatial locality applies to this scenario since FUSE will most probably use more space ahead). Apart from the space inefficiency, increasing the file block size will also harm the DHT (since it was not built for storing large files). Regarding the write throttling, it should be configurable according to the available network bandwidth (it can be a user configuration parameter). For the prototype we are delivering we use file blocks of 16KB and 10 writes per second, 160KB/s (this value was obtained after testing multiple values).

\subsection{File System Metadata}
Within our DHT, we only store two types of objects: metadata objects and file blocks. The first represents all the metadata for one user. This introduces a big simplification in our implementation (since only one object has all the needed metadata) and will also be more efficient for file accessing (since we only need to access the metadata and then blocks). Using multiple metadata objects one would have to do multiple accesses (to the DHT) to actually access some block. We believe that using a single metadata object is the best approach if we take into consideration that metadata objects will not grow very much (we could confirm it while testing the system).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section:  Experimental Evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Evaluation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section:  Conclusions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}


\end{document}
